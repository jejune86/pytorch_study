
백본
- 입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 모델의 일부
- VGG, ResNet, Mask R-CNN 논문 등에서 직간접적으로 언급됨

- 백본 네트워크는 입력 데이터에서 특징을 추출하므로 노이즈와 불필요한 특성 제거하고, 중요한 특징 추출 가능
  -> 이렇게 추출된 특징을 활용해 새로운 모델이나 기능의 입력으로 사용

ex)
객체 검출 CNN
- 초기 계층에서 점이나 선 같은 저수준의 특징 학습
- 중간에선 객체나 형태
- 상위에선 이전 계틍 특징 기반으로 객체 이해 검출

-> 확장하여 (백본으로 사용하여) 최종 계층을 바꿔 포즈 추정, 이미지 분할 등의 모델 구성 가능 

- 백본을 활용한다고, 모델의 성능이 급격하게 좋아지지는 않음
- 백본으로 사용하는 딥러닝 모델은 많은 수의 매개변수가 존재하여 overfitting 되기 쉬움
  -> regularization, Normalization 사용 권장

- 자연어 처리와 컴퓨터비전 작업에서 백본이 되는 모델은 BERT, GPT, VGG-16, ResNet과 같이 초대규모 딥러닝 모델 사용

전이 학습 (Transfer Learning)
- 어떤 작업을 수행하기 위해 이미 사전 학습된 모델을 재사용해 새로운 작업이나 관련 도메인의 성능을 향상시킬 수 있는 기술
- Source Domain : 사전 학습된 모델이 학습에 사용한 도메인
- Target Domain : 전이 학습에 사용될 도메인
- 다시 말해, 사전 학습된 모델을 활용해 현재 시스템에 맞는 새로운 모델로 학습하는 과정

- Upstream : 사전 학습된 모델 영역
  - 전이 학습 파이프라인 중 시작 부분에 위치
- Downstream : 미세 조정된 모델 영역
  - 파이프라인 마지막에 위치


귀납적 전이 학습 (Inductive Transfer Learning)
- 기존에 학습한 모델의 지식을 활용하여 새로운 작업을 수행하기 위한 방법
- 작업 효율성 높이고 성능 향상, 일반화 능력 향상
- 자기 주도적 학습(Self-Taught)과, 다중 작업 학습(Multi-Task)으로 나뉨

- 자기 주도적 학습
  - 비지도 전이 학습의 유형 중 하나
  - 데이터 양이 많으나, label 된 데이터가 적거나 없을 때 사용
  - auto encoder 같은 모델을 학습시켜, 저차원 공간에서 label된 데이터로 미세조정

- 다중 작업 학습
  - 레이블이 지정된 소스 도메인과 타깃 도메인 데이터를 기반으로 모델에 여러 작업을 동시에 가리치는 방법
  - 공유 계층(Shared Layer)과 작업별 계층(Task Specific Layer)으로 나뉨
  - 공유 계층
    - 소스 도메인과 타깃 도메인 둘 다의 데이터 세트에서 모델 사전 학습
    - 단일 작업을 위해 작업별 계층마다 타깃 도메인 데이터 세트로 미세 조정
    - 작업마다 서로 다른 학습 데이터세트를 사용하여 모델을 미세 조정
    - 서로 다른 작업의 특징을 맞추기 위해 동시에 학습되어 하나의 작업에 overfitting되지 않음
    - 서로의 작업이 동일한 도메인을 사용하므로 성능 향상에 기여

변환적 전이 학습 (Transductive Trnasfer Learning)
- 소스 도메인과 타깃 도메인이 유사하지만 완전히 동일하지 않은 경우
- 소스 도메인은 레이블이 존재하며, 타깃 도메인에는 존재하지 않은 경우에 사용

- 도메인 적응(Domain Adaptation)과 표본 선택 편향/공변량 이동(Sample Selection Bias/Covariance Shift)로 나눈다.

- 도메인 적응
  - 소스 도메인과 타깃 도메인의 특징 분포(Feature Distribution)를 전이시키는 방법

- 표본 선택 편향/공변량 이동
  - 소스 도메인과 타깃 도메인의 분산과 편향이 크게 다를 때 표본을 선택해 편향, 공변량을 이동시키는 방법
  - 소스 도메인과 타깃 도메인의 차이로, overfitting 가능
    -> 무작위/비무작위 샘플링 방법이나 도메인 적응을 통해 해당 학습치만 전이

비지도 전이 학습 (Unsupervised Transfer Learning)
- 소스, 타깃 도메인 둘 다 label 없을 때 
- 소스 도메인에서 타깃 도메인의 성능을 개선하는 데 사용할 수 있는 특징 표현을 학습
- 소스 도메인 데이터에서 Unseupervised model을 교육해 일련의 기능 표현을 학습
  -> 타깃 도메인에 대한 감독된 모델을 초기화하는 방법

- 레이블의 영향을 받지 않고 데이터가 가진 특징을 학습했으므로, 미세 조정 시 더 효과적으로 타깃 도메인에 대해 예측 수행 가능
- 대표적인 방법
  - 생성적 적대 신경망 (Generative Adversarial Networks, GAN), Clustering


전이 학습은 사전 학습된 모델의 지식을 활용하므로 작은 데이터 세트를 가지고도 우수한 결과 얻을 수 있음


제로-샷 전이 학습 (Zero-shot Transfer Learning)
- 사전 학습된 모델을 이용해 다른 도메인에서도 적용할 수 있는 기법
ex)
- (독수리, 새), (참새, 새), (오리 새) 등의 데이터 쌍으로 모델 학습 시키고, 부엉이 같은 새로운 이미지 분류에서도 성능 발휘

- 새로운 도메인에서 학습할 데이터가 부족한 경우에 유용하게 사용 가능 

원-샷 전이 학습 (One-shot Transfer Learning)
- 제로-샷과 비슷하지만, 한 번에 하나의 샘플만 사용해 모델 학습
  -> 매우 적은 양의 데이터를 이용하여 classification 가능
- 서포트 셋 (Support set)
  - 학습에 사용될 클래스의 대표 샘플, 하나 이상의 대표 샘플로 이뤄진다.
- 쿼리 셋 (Query Set)
  - 새로운 클래스를 분류하기 위한 입력 데이터, 분류 대상 데이터, 서포트 셋에서 수집한 샘플과는 다른 샘플이어야 함
- 서포트 셋에 있는 대표 샘플과 쿼리 셋 간의 거리 측정하여, 쿼리셋과 가장 가까운 서포트 셋의 대표 샘플의 클래스로 분류
  -> Euclid Distance, Cosine Similarity

ex) 개 고양이 분류 문제
- 개 클래스와 고양이 클래스 각각 대표 샘플 수집하여 서포트 셋 생성
- 각 클래스의 대표 샘플은 개 사진과 고양이 사진 중 하나로 선택
- 서포트 셋에 있는 대표 샘플과 쿼리 셋 간의 거리 측정하여 가까운 쪽으로 분류

-------유형-------------세부유형--------------소스 도메인----------타깃 도메인-------
귀납적 전이 학습 |   자기 주도적 학습   | 레이블이 없는 데이터 | 레이블이 있는 데이터
다중 작업 학습   | 레이블이 있는 데이터 | 레이블이 있는 데이터 | 레이블이 있는 데이터
변환적 전이 학습 |           -         | 레이블이 있는 데이터 | 레이블이 있는 데이터
비지도 전이 학습 |           -         | 레이블이 없는 데이터 | 레이블이 없는 데이터
제로-샷 전이 학습|           -         | 레이블이 없는 데이터 | 레이블이 없는 데이터
원-샷 전이 학습  |           -         | 레이블이 없는 데이터 | 레이블이 없는 데이터


특징 추출 (Feature Extraction) 및 미세 조정 (Fine-tuning)
- 대규모 데이터셋으로 사전 학습된 모델을 작은 데이터 셋으로 추가학습해 가중치, 편향 수정
- 특징 추출
  - 타깃 도메인이 소스 도메인과 유사하고 타깃 도메인의 데이터 세트가 적을 때 사용
  - 굳이 Freeze 안하고, 출력만 바꿔서 학습

- 미세 조정
  - 특징 추출 계층을 일부만 동결하거나 동결하지 않고 타깃 도메인에 대한 학습 진행
  - 전략 세가지
    1. 소스 도메인, 타깃 도메인 유사성 낮을 때 (개 vs 고양이)
        - 백본 네트워크의 일부 계층 동결하지 않고 전체 네트워크 학습
    2. 도메인 간 유사성 매우 낮고, 데이터 세트 크기가 작을 때
        - 일부 계층만 동결해 학습 진행, 하위 계층에서 저수준의 특징 학습할 때 동일한 특징으로 학습될 가능성 높으므로 초기 계층만 동결

    3. 소스 도메인, 타깃 도메인 유사성 높고, 데이터 세트 크기가 작을 때
        - 하위 계층을 동결하고 상위 기ㅖ층을 학습하는 방법으로



